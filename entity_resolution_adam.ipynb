{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ama = pd.read_csv('./data/amazon.csv')\n",
    "rt = pd.read_csv('./data/rotten_tomatoes.csv')\n",
    "\n",
    "holdout = pd.read_csv('./data/holdout.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "train = pd.read_csv('./data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ama.columns = [\n",
    "    'id_left',\n",
    "    'time_left',\n",
    "    'director_left',\n",
    "    'star_left',\n",
    "    'cost_left'\n",
    "]\n",
    "\n",
    "rt.columns = [\n",
    "    'id_right',\n",
    "    'time_right',\n",
    "    'director_right',\n",
    "    'year_right',\n",
    "    'star1_right',\n",
    "    'star2_right',\n",
    "    'star3_right',\n",
    "    'star4_right',\n",
    "    'star5_right',\n",
    "    'star6_right',\n",
    "    'rotten_tomatoes_right',\n",
    "    'audience_rating_right',\n",
    "    'review1_right',\n",
    "    'review2_right',\n",
    "    'review3_right',\n",
    "    'review4_right',\n",
    "    'review5_right'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge records from ama and rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_features(dataset):\n",
    "    new_data = pd.DataFrame(dtype=str)\n",
    "    \n",
    "    for index, row in dataset.iterrows():\n",
    "        amarow = ama[ama['id_left'] == row['id1']]\n",
    "        rtrow = rt[rt['id_right'] == row['id 2']]\n",
    "\n",
    "        amarow.reset_index(drop=True, inplace=True)\n",
    "        rtrow.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        new_row = pd.concat([amarow, rtrow], axis=1)\n",
    "        new_data = pd.concat([new_data, new_row])\n",
    "    \n",
    "    new_data.fillna('0', inplace = True)\n",
    "    new_data.dropna()\n",
    "    \n",
    "    # Compute directors match column\n",
    "    new_data['directors_same'] = (new_data['director_left'] == new_data['director_right']).astype(int)\n",
    "    \n",
    "    # Compute time columns\n",
    "    new_data['time_left'] = new_data['time_left'].astype(str)\n",
    "    new_data['time_right'] = new_data['time_right'].astype(str)\n",
    "    new_data['time_norm_left'] = new_data['time_left'].apply(compute_time_norm)\n",
    "    new_data['time_norm_right'] = new_data['time_right'].apply(compute_time_norm)\n",
    "    new_data['time_same'] = (new_data['time_norm_left'].astype(int) == new_data['time_norm_right'].astype(int)).astype(int)\n",
    "    new_data['time_diff'] = (new_data['time_norm_left'].astype(int) - new_data['time_norm_right'].astype(int)).astype(int)\n",
    "    \n",
    "    # Compute actors columns\n",
    "    actors_split = new_data['star_left'].str.split(', ', expand=True)\n",
    "    for i in range(6 - actors_split.shape[1]):\n",
    "        actors_split[str(i)] = \"\"\n",
    "    actors_split.columns = ['star_' + str(i) for i in range(6)]\n",
    "    new_data = pd.concat([new_data, actors_split], axis=1)\n",
    "    \n",
    "    # Number actors match\n",
    "    cols = list(new_data.loc[:,'star_0':'star_4']) + list(new_data.loc[:,'star1_right':'star6_right'])\n",
    "    new_data['num_match_stars'] = new_data[cols].apply(compute_number_stars_match, axis = 1)\n",
    "    \n",
    "    # Percent actors match\n",
    "    new_data['percent_match_stars'] = new_data[cols].apply(compute_percent_stars_match, axis = 1)\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_number_stars_match(row):\n",
    "    actors_left = ['star_0', 'star_1', 'star_2', 'star_3', 'star_4']\n",
    "    actors_right = ['star1_right', 'star2_right', 'star3_right', 'star4_right', 'star5_right', 'star6_right']\n",
    "    list_left = row.loc[actors_left].tolist()\n",
    "    list_right = row.loc[actors_right].tolist()\n",
    "    \n",
    "    # Avoid matching nulls\n",
    "    list_left = filter(None, list_left)\n",
    "    list_right = filter(None, list_right)\n",
    "    \n",
    "    return len(np.intersect1d(list_left, list_right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_percent_stars_match(row):\n",
    "    actors_left = ['star_0', 'star_1', 'star_2', 'star_3', 'star_4']\n",
    "    actors_right = ['star1_right', 'star2_right', 'star3_right', 'star4_right', 'star5_right', 'star6_right']\n",
    "    list_left = row.loc[actors_left].tolist()\n",
    "    list_left = filter(None, list_left)\n",
    "    list_right = row.loc[actors_right].tolist()\n",
    "    x = float(len(np.intersect1d(list_left, list_right)))\n",
    "    ama_num = len(list_left)\n",
    "    return x / ama_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regex = re.compile(r'[0-9]*')\n",
    "\n",
    "def compute_time_norm(row):\n",
    "    row = str(row)\n",
    "    \n",
    "    if '/' in row:\n",
    "        # Invalid time entry\n",
    "        print(row)\n",
    "        return 0\n",
    "    \n",
    "    match = regex.findall(row)\n",
    "    temp = filter(None, match)\n",
    "    if len(temp) == 2:\n",
    "        return 60*int(temp[0]) + int(temp[1])\n",
    "    if len(temp) == 1:\n",
    "        return temp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 981,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91935483870967738"
      ]
     },
     "execution_count": 981,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_cols = ['time_diff', 'directors_same', 'num_match_stars', 'percent_match_stars', 'year_right', 'rotten_tomatoes_right', 'audience_rating_right']\n",
    "\n",
    "train_data = create_features(train)\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_data = pd.concat([train_data, train], axis=1) #, how='inner')\n",
    "\n",
    "# Remove bad training rows\n",
    "train_data = train_data[train_data['id_left'] != 199]\n",
    "train_data = train_data[train_data['id_left'] != 680]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_data[features_cols], train_data['gold'])\n",
    "\n",
    "clf = GradientBoostingClassifier(min_samples_split=10)\n",
    "clf.fit(x_train, y_train)\n",
    "clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error inds: [16 21]\n",
      "Preds: [0 0]\n",
      "Labels: [1 1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_diff</th>\n",
       "      <th>directors_same</th>\n",
       "      <th>num_match_stars</th>\n",
       "      <th>percent_match_stars</th>\n",
       "      <th>year_right</th>\n",
       "      <th>rotten_tomatoes_right</th>\n",
       "      <th>audience_rating_right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>-7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>43</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     time_diff  directors_same  num_match_stars  percent_match_stars  \\\n",
       "154         -7               1                2                  1.0   \n",
       "226         14               1                1                  1.0   \n",
       "\n",
       "     year_right rotten_tomatoes_right audience_rating_right  \n",
       "154        2010                    43                    58  \n",
       "226        2012                     0                    80  "
      ]
     },
     "execution_count": 968,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=200)\n",
    "clf.fit(x_train, y_train)\n",
    "preds = clf.predict(x_test)\n",
    "\n",
    "# Missed preds:\n",
    "errors = preds - y_test\n",
    "error_ind = np.nonzero(errors)[0]\n",
    "\n",
    "print(\"Error inds: \" + str(error_ind))\n",
    "print(\"Preds: \" + str(preds[error_ind]))\n",
    "print(\"Labels: \" + str(y_test.as_matrix()[error_ind]))\n",
    "\n",
    "rel_cols = ['director_left', 'time_right', 'directors_same', 'time_same', 'time_diff', 'num_match_stars', 'gold']\n",
    "x_test.iloc[error_ind]\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = create_features(train)\n",
    "test_data = create_features(test)\n",
    "\n",
    "\n",
    "# Remove bad training rows\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "train_data = pd.concat([train_data, train], axis=1)\n",
    "train_data = train_data[train_data['id_left'] != 199]\n",
    "train_data = train_data[train_data['id_left'] != 680]\n",
    "\n",
    "\n",
    "clf = GradientBoostingClassifier(min_samples_split=10)\n",
    "clf.fit(train_data[features_cols], train_data['gold'])\n",
    "test_preds = clf.predict(test_data[features_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_preds = pd.DataFrame(test_preds)\n",
    "test_preds.columns = ['gold']\n",
    "test_preds.to_csv('test_gold.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with Holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = create_features(train)\n",
    "holdout_data = create_features(holdout)\n",
    "\n",
    "# Remove bad training rows\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "train_data = pd.concat([train_data, train], axis=1)\n",
    "train_data = train_data[train_data['id_left'] != 199]\n",
    "train_data = train_data[train_data['id_left'] != 680]\n",
    "\n",
    "clf = GradientBoostingClassifier(min_samples_split=10)\n",
    "clf.fit(train_data[features_cols], train_data['gold'])\n",
    "holdout_preds = clf.predict(holdout_data[features_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 985,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "holdout_preds = pd.DataFrame(holdout_preds)\n",
    "holdout_preds.columns = ['gold']\n",
    "holdout_preds.to_csv('holdout_gold.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

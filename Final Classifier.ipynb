{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon and Rotten Tomatoes Entity Resolution\n",
    "### Adam Coviensky (ac4092), Ian Johnson (icj2103)\n",
    "### Instabase submission: icj2103\n",
    "### GitHub repo: https://github.com/adamianaml/aml-entity-resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our entity resolution technique involved joining the Amazon and Rotten Tomatoes databases for the ID’s listed in the training set. We then matched the schemas for the features contained in both of the datasets and created new features from these. These features included the absolute value of the difference in time. Boolean features indicating whether or not the directors of the two movies are the same, the runtimes are the same, and whether or not the runtimes are the same within some range (we used 3 minutes). We then created features for the number of matching actors and for the percentage of matching actors. Finally, we performed cross-validation with a customized gridsearch using the training set to determine which features we should be keeping in our final model and which classifier to use between a decision tree, a random forest, and the gradient boosting classifier.\n",
    " \n",
    "Our model had a precision, recall, and F1-scores of 0.97, 0.97 and 0.96 respectively for the training set. We obtained these scores by using train_test_split on our training set and fitting the model to the training portion and testing it on the testing portion.\n",
    " \n",
    " \n",
    "The most important features were determined to be ‘time_same_2’ which is a binary variable indicating whether the two runtimes are equivalent within a range of 3 minutes. Also, ‘directors_same’, indicated whether the two directors are the same.  ‘num_match_stars’ and ‘percent_match_stars’ indicated the number of matching stars and the percentage of the stars matched from the Amazon dataset respectively. These were the features we used to train our final model.\n",
    " \n",
    "We avoided pairwise comparison of all movies across both datasets by simply joining the id’s listed in the training set to create a model. We trained a random forest classifier on this training data with our newly created features. This allowed us to predict whether two new movies matched using our trained random forest on a new combination of two movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "ama = pd.read_csv('./data/amazon.csv')\n",
    "rt = pd.read_csv('./data/rotten_tomatoes.csv')\n",
    "\n",
    "holdout = pd.read_csv('./holdout.csv')\n",
    "test = pd.read_csv('./test.csv')\n",
    "train = pd.read_csv('./train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ama.columns = [\n",
    "    'id_left',\n",
    "    'time_left',\n",
    "    'director_left',\n",
    "    'star_left',\n",
    "    'cost_left'\n",
    "]\n",
    "\n",
    "rt.columns = [\n",
    "    'id_right',\n",
    "    'time_right',\n",
    "    'director_right',\n",
    "    'year_right',\n",
    "    'star1_right',\n",
    "    'star2_right',\n",
    "    'star3_right',\n",
    "    'star4_right',\n",
    "    'star5_right',\n",
    "    'star6_right',\n",
    "    'rotten_tomatoes_right',\n",
    "    'audience_rating_right',\n",
    "    'review1_right',\n",
    "    'review2_right',\n",
    "    'review3_right',\n",
    "    'review4_right',\n",
    "    'review5_right'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Creation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_features(dataset):\n",
    "    new_data = pd.DataFrame(dtype=str)\n",
    "\n",
    "    for index, row in dataset.iterrows():\n",
    "        amarow = ama[ama['id_left'] == row['id1']]\n",
    "        rtrow = rt[rt['id_right'] == row['id 2']]\n",
    "\n",
    "        amarow.reset_index(drop=True, inplace=True)\n",
    "        rtrow.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        new_row = pd.concat([amarow, rtrow], axis=1)\n",
    "        new_data = pd.concat([new_data, new_row])\n",
    "\n",
    "    new_data.fillna('0', inplace = True)\n",
    "    new_data.dropna()\n",
    "\n",
    "    # Compute directors match column\n",
    "    new_data['directors_same'] = (new_data['director_left'] == new_data['director_right']).astype(int)\n",
    "\n",
    "    # Compute time columns\n",
    "    new_data['time_left'] = new_data['time_left'].astype(str)\n",
    "    new_data['time_right'] = new_data['time_right'].astype(str)\n",
    "    new_data['time_norm_left'] = new_data['time_left'].apply(compute_time_norm)\n",
    "    new_data['time_norm_right'] = new_data['time_right'].apply(compute_time_norm)\n",
    "    new_data['time_same'] = (new_data['time_norm_left'].astype(int) == new_data['time_norm_right'].astype(int)).astype(int)\n",
    "    new_data['time_same_2'] = (abs((new_data['time_norm_left'].astype(int) - new_data['time_norm_right'].astype(int)).astype(int)) <= 3).astype(int)\n",
    "    new_data['time_diff'] = (new_data['time_norm_left'].astype(int) - new_data['time_norm_right'].astype(int)).astype(int)\n",
    "    new_data['time_diff_abs'] = abs(new_data['time_diff']).astype(int)\n",
    "\n",
    "    # Compute actors columns\n",
    "    actors_split = new_data['star_left'].str.split(', ', expand=True)\n",
    "    for i in range(6 - actors_split.shape[1]):\n",
    "        actors_split[str(i)] = \"\"\n",
    "    actors_split.columns = ['star_' + str(i) for i in range(6)]\n",
    "    new_data = pd.concat([new_data, actors_split], axis=1)\n",
    "\n",
    "    # Number actors match\n",
    "    cols = list(new_data.loc[:,'star_0':'star_4']) + list(new_data.loc[:,'star1_right':'star6_right'])\n",
    "    new_data['num_match_stars'] = new_data[cols].apply(compute_number_stars_match, axis = 1)\n",
    "\n",
    "    # Percent actors match\n",
    "    new_data['percent_match_stars'] = new_data[cols].apply(compute_percent_stars_match, axis = 1)\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Feature Creation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_number_stars_match(row):\n",
    "    actors_left = ['star_0', 'star_1', 'star_2', 'star_3', 'star_4']\n",
    "    actors_right = ['star1_right', 'star2_right', 'star3_right', 'star4_right', 'star5_right', 'star6_right']\n",
    "    list_left = row.loc[actors_left].tolist()\n",
    "    list_right = row.loc[actors_right].tolist()\n",
    "\n",
    "    # Avoid matching nulls\n",
    "    list_left = filter(None, list_left)\n",
    "    list_right = filter(None, list_right)\n",
    "\n",
    "    return len(np.intersect1d(list_left, list_right))\n",
    "\n",
    "\n",
    "def compute_percent_stars_match(row):\n",
    "    actors_left = ['star_0', 'star_1', 'star_2', 'star_3', 'star_4']\n",
    "    actors_right = ['star1_right', 'star2_right', 'star3_right', 'star4_right', 'star5_right', 'star6_right']\n",
    "    list_left = row.loc[actors_left].tolist()\n",
    "    list_left = filter(None, list_left)\n",
    "    list_right = row.loc[actors_right].tolist()\n",
    "    x = float(len(np.intersect1d(list_left, list_right)))\n",
    "    ama_num = len(list_left)\n",
    "    return x / ama_num\n",
    "\n",
    "\n",
    "regex = re.compile(r'[0-9]*')\n",
    "\n",
    "def compute_time_norm(row):\n",
    "    row = str(row)\n",
    "\n",
    "    if '/' in row:\n",
    "        # Invalid time entry\n",
    "        print(row)\n",
    "        return 0\n",
    "\n",
    "    match = regex.findall(row)\n",
    "    temp = filter(None, match)\n",
    "    if len(temp) == 2:\n",
    "        return 60*int(temp[0]) + int(temp[1])\n",
    "    if len(temp) == 1:\n",
    "        return temp[0]\n",
    "\n",
    "\n",
    "def remove_bad_samples(data):\n",
    "\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data = pd.concat([data, train], axis=1)\n",
    "\n",
    "    # Remove bad training rows\n",
    "    data = data[data['id_left'] != 199]\n",
    "    data = data[data['id_left'] != 680]\n",
    "    data = data[data['id_left'] != 487]\n",
    "    data = data[data['id_left'] != 756]\n",
    "    data = data[data['id_left'] != 770]\n",
    "    data = data[data['id_left'] != 1701]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict With Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98        56\n",
      "          1       1.00      0.60      0.75         5\n",
      "\n",
      "avg / total       0.97      0.97      0.96        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_cols = [\n",
    "    'time_same_2',\n",
    "    'directors_same',\n",
    "    'num_match_stars',\n",
    "    'percent_match_stars',\n",
    "]\n",
    "\n",
    "train_data = create_features(train)\n",
    "train_data = remove_bad_samples(train_data)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_data[features_cols], train_data['gold'], random_state=10)\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(x_train, y_train)\n",
    "scores = clf.score(x_test, y_test)\n",
    "\n",
    "preds = clf.predict(x_test)\n",
    "\n",
    "# F1 score:\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_cols = [\n",
    "    'time_same_2',\n",
    "    'directors_same',\n",
    "    'num_match_stars',\n",
    "    'percent_match_stars',\n",
    "]\n",
    "\n",
    "\n",
    "# Test Predictions\n",
    "train_data = create_features(train)\n",
    "train_data = remove_bad_samples(train_data)\n",
    "test_data = create_features(test)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(train_data[features_cols], train_data['gold'])\n",
    "test_preds = clf.predict(test_data[features_cols])\n",
    "\n",
    "test_preds = pd.DataFrame(test_preds)\n",
    "test_preds.columns = ['gold']\n",
    "test_preds.to_csv('test_gold.csv', index=False)\n",
    "\n",
    "\n",
    "# Holdout Predictions\n",
    "train_data = create_features(train)\n",
    "train_data = remove_bad_samples(train_data)\n",
    "holdout_data = create_features(holdout)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(train_data[features_cols], train_data['gold'])\n",
    "holdout_preds = clf.predict(holdout_data[features_cols])\n",
    "\n",
    "holdout_preds = pd.DataFrame(holdout_preds)\n",
    "holdout_preds.columns = ['gold']\n",
    "holdout_preds.to_csv('holdout_gold.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
